clear, clc, close all

fprintf('--- PHASE 1: Setting up Universal DF-CNN Training ---\n');
modulation_type = 'psk';
modulation_order = 2;
phase_offset = 0;
tau = 1.0;
window_len = 31;
num_feedback_taps = 4;
input_len = window_len * 2 + num_feedback_taps;

k = log2(modulation_order);
fprintf('Configuration: %d-%s, phase=%.3f rad, tau=%.1f\n', modulation_order, upper(modulation_type), phase_offset, tau);

fprintf('--- PHASE 2: Defining Simple Working DF-CNN Architecture ---\n');

lgraph = layerGraph();

input_layer = featureInputLayer(input_len, 'Name', 'combined_input', 'Normalization', 'zscore');
lgraph = addLayers(lgraph, input_layer);

deep_layers = [
    fullyConnectedLayer(256, 'Name', 'fc1')
    reluLayer('Name', 'relu1')
    batchNormalizationLayer('Name', 'bn1')
    
    fullyConnectedLayer(128, 'Name', 'fc2')
    reluLayer('Name', 'relu2')
    batchNormalizationLayer('Name', 'bn2')
    dropoutLayer(0.3, 'Name', 'dropout1')
    
    fullyConnectedLayer(64, 'Name', 'fc3')
    reluLayer('Name', 'relu3')
    dropoutLayer(0.4, 'Name', 'dropout2')
    
    fullyConnectedLayer(modulation_order, 'Name', 'fc_final')
    softmaxLayer('Name', 'softmax')
    classificationLayer('Name', 'classoutput')
];
lgraph = addLayers(lgraph, deep_layers);

lgraph = connectLayers(lgraph, 'combined_input', 'fc1');

fprintf('Simple DF-CNN architecture created for %d-class classification\n', modulation_order);

fprintf('\n--- PHASE 3: Generating training data with mixed decision feedback ---\n');
N_train = 150000;
SNR_train_dB = 8;
teacher_forcing_ratio = 0.3;

[x_train, y_train] = generate_realistic_df_data(N_train, tau, SNR_train_dB, window_len, num_feedback_taps, modulation_type, modulation_order, phase_offset, teacher_forcing_ratio);
Y_cat_train = categorical(y_train);

fprintf('Generated %d training samples (teacher forcing: %.1f%%)\n', size(x_train, 1), teacher_forcing_ratio*100);
for i = 0:modulation_order-1
    fprintf('  Symbol %d: %d samples\n', i, sum(y_train==i));
end

fprintf('\n--- PHASE 4: Training the Simple DF-CNN...\n');
cv = cvpartition(size(x_train, 1), 'HoldOut', 0.15);
X_train_final = x_train(cv.training, :);
Y_train_final = Y_cat_train(cv.training, :);
X_val = x_train(cv.test, :);
Y_val = Y_cat_train(cv.test, :);

options = trainingOptions('adam', ...
    'MaxEpochs', 20, ...
    'MiniBatchSize', 512, ...
    'InitialLearnRate', 0.001, ...
    'LearnRateSchedule', 'piecewise', ...
    'LearnRateDropFactor', 0.5, ...
    'LearnRateDropPeriod', 5, ...
    'ValidationData', {X_val, Y_val}, ...
    'ValidationFrequency', 200, ...
    'Plots', 'training-progress', ...
    'Verbose', true, ...
    'Shuffle', 'every-epoch', ...
    'ValidationPatience', 10, ...
    'L2Regularization', 0.0001);

universal_df_model = trainNetwork(X_train_final, Y_train_final, lgraph, options);

fprintf('\n--- PHASE 5: Saving the Universal DF-CNN model...\n');
if ~exist('mat/universal_df', 'dir'), mkdir('mat/universal_df'); end
phase_str = strrep(sprintf('%.3f', phase_offset), '.', 'p');
fname = sprintf('mat/universal_df/simple_df_%s%d_tau%02d_phase%s.mat', ...
    modulation_type, modulation_order, tau*10, phase_str);
save(fname, 'universal_df_model', 'modulation_type', 'modulation_order', 'phase_offset', 'tau', 'window_len', 'num_feedback_taps');
fprintf('Model saved to: %s\n', fname);

fprintf('\n--- PHASE 6: Quick performance validation...\n');
y_pred_probs = predict(universal_df_model, X_val);
[~, y_pred_idx] = max(y_pred_probs, [], 2);
y_pred = y_pred_idx - 1;

fprintf('DEBUG INFO:\n');
fprintf('  y_pred_probs size: [%d, %d]\n', size(y_pred_probs));
fprintf('  y_pred range: %d to %d\n', min(y_pred), max(y_pred));
fprintf('  Y_val type: %s\n', class(Y_val));

accuracy = mean(y_pred == (double(Y_val) - 1)) * 100;
ser = mean(y_pred ~= (double(Y_val) - 1)) * 100;

fprintf('Validation Results:\n');
fprintf('  Symbol Accuracy: %.2f%%\n', accuracy);
fprintf('  Symbol Error Rate: %.4f%%\n', ser);

if ser < 1.0
    fprintf('SUCCESS: Simple DF-CNN achieved excellent performance!\n');
elseif ser < 5.0
    fprintf('GOOD: Simple DF-CNN shows promising results\n');
else
    fprintf('WARNING: May need more training or check data\n');
end

function [x, y] = generate_realistic_df_data(N, tau, SNR_dB, win_len, num_taps, mod_type, M, phase, teacher_forcing_ratio)
    k = log2(M);
    constellation = generate_constellation(M, mod_type, phase);
    
    symbol_indices = randi([0 M-1], N, 1);
    symbols = constellation(symbol_indices + 1);
    
    is_real_modulation = (M == 2) && strcmpi(mod_type, 'psk');
    
    sps=10; beta=0.3; span=6; h=rcosdesign(beta,span,sps,'sqrt');
    h = h / norm(h);
    tx_up = upsample(symbols, round(sps*tau));
    txSignal = conv(tx_up, h);
    
    snr_eb_n0 = 10^(SNR_dB/10);
    snr_es_n0 = k * snr_eb_n0;
    noise_variance = 1 / (round(sps*tau) * snr_es_n0);
    
    if is_real_modulation
        noise = sqrt(noise_variance) * randn(size(txSignal));
    else
        noise = sqrt(noise_variance/2) * (randn(size(txSignal)) + 1j*randn(size(txSignal)));
    end
    
    rxSignal = txSignal + noise;
    rxMF = conv(rxSignal, h);
    delay = finddelay(tx_up, rxMF);
    
    x = zeros(N, win_len*2 + num_taps);
    y = zeros(N, 1);
    half_win = floor(win_len/2);
    
    decision_history = zeros(num_taps, 1);
    
    for i = (num_taps + 1):N
        loc = round((i-1)*sps*tau) + 1 + delay;
        if (loc > half_win) && (loc + half_win <= length(rxMF))
            win_complex = rxMF(loc-half_win:loc+half_win);
            
            if is_real_modulation
                win_features = [real(win_complex(:))', real(win_complex(:))'];
            else
                win_features = [real(win_complex(:))', imag(win_complex(:))'];
            end
            
            if rand() < teacher_forcing_ratio
                past_symbols = symbol_indices(i-1:-1:i-num_taps);
            else
                past_symbols = decision_history;
            end
            
            if i > num_taps + 1
                win_with_past = rxMF(loc-half_win:loc+half_win);
                if is_real_modulation
                    features_temp = [real(win_with_past(:))', real(win_with_past(:))'];
                else
                    features_temp = [real(win_with_past(:))', imag(win_with_past(:))'];
                end
                input_temp = [features_temp, decision_history'];
                
                pred_prob_temp = rand(1, M);
                pred_prob_temp = pred_prob_temp / sum(pred_prob_temp);
                [~, pred_idx_temp] = max(pred_prob_temp);
                predicted_sym = pred_idx_temp - 1;
                
                decision_history = [predicted_sym; decision_history(1:end-1)];
            else
                decision_history = [symbol_indices(i-1); decision_history(1:end-1)];
            end
            
            x(i, :) = [win_features, past_symbols'];
            y(i) = symbol_indices(i);
        end
    end
    
    valid = any(x,2);
    x = x(valid,:);
    y = y(valid,:);
end

function constellation = generate_constellation(M, type, phase)
    if strcmpi(type, 'psk')
        p = 0:M-1;
        constellation = exp(1j*(2*pi*p/M + phase));
    elseif strcmpi(type, 'qam')
        k = log2(M);
        if mod(k,2) ~= 0
            error('QAM order must be a power of 4 (4, 16, 64, ...)');
        end
        n = sqrt(M);
        vals = -(n-1):2:(n-1);
        [X,Y] = meshgrid(vals, vals);
        constellation = (X + 1j*Y) * exp(1j*phase);
        constellation = constellation(:);
    else
        error('Unknown modulation type. Use "psk" or "qam".');
    end
    
    constellation = constellation / sqrt(mean(abs(constellation).^2));
end